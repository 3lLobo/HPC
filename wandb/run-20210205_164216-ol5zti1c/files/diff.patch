diff --git a/ml_assignment/cifar10_hvd_2.py b/ml_assignment/cifar10_hvd_2.py
index 1a8f3e4..0a948fc 100644
--- a/ml_assignment/cifar10_hvd_2.py
+++ b/ml_assignment/cifar10_hvd_2.py
@@ -1,11 +1,12 @@
 import tensorflow as tf
 from tensorflow.keras.models import Sequential
-from tensorflow.keras.layers import Dense, Dropout, Flatten
+from tensorflow.keras.layers import Dense, Activation
 from tensorflow.keras.layers import Conv2D, MaxPooling2D
 from tensorflow.keras import backend as K
 import math
 import horovod.tensorflow.keras as hvd
 import wandb
+from wandb.keras import WandbCallback
 from time import time
 from tensorflow.keras.callbacks import TensorBoard
 
@@ -57,16 +58,20 @@ print(x_test.shape[0], 'test samples')
 # model.add(Dropout(0.5))
 # model.add(Dense(num_classes, activation='softmax'))
 
-model = tf.keras.applications.EfficientNetB0(
+effNet = tf.keras.applications.EfficientNetB0(
     include_top=True,
     weights="imagenet",
     input_tensor=None,
     input_shape=None,
     pooling=None,
-    classes=num_classes,
-    classifier_activation="softmax",
+    classes=1000,
+    classifier_activation=None,
     )
 
+model = Sequential()
+model.add(effNet)
+model.add(Dense(num_classes, activation='softmax'))
+
 # Horovod: adjust learning rate based on number of GPUs.
 opt = tf.keras.optimizers.Adadelta(1.0 * hvd.size())
 
@@ -83,14 +88,13 @@ callbacks = [
     # This is necessary to ensure consistent initialization of all workers when
     # training is started with random weights or restored from a checkpoint.
     hvd.callbacks.BroadcastGlobalVariablesCallback(0),
-    tensorboard
-]
+    WandbCallback, tensorboard, 
+    ]
 
 # Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them.
 # if hvd.rank() == 0:
 #     callbacks.append(keras.callbacks.ModelCheckpoint('./checkpoint-{epoch}.h5'))
 
-wandb.watch(model)
 model.fit(x_train, y_train,
           batch_size=batch_size,
           callbacks=callbacks,
